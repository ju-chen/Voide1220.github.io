---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

<span class='anchor' id='about-me'></span>

Hello! Iâ€™m Chen Ju (é é™ˆ).

I'm a PhD candidate at MediaBrain Group, Cooperative Medianet Innovation Center, Shanghai Jiao Tong University, supervised by <a href="https://sites.google.com/site/zhixuli">Prof. Ya Zhang</a>, also advised by <a href="https://galina0217.github.io/">Prof. Weidi Xie</a> and <a href="http://web.suda.edu.cn/jjf2/">Prof. Siheng Chen</a>.  

Currently, I closely collaborate with researchers from Huawei Cloud: <a href="https://scholar.google.com/citations?user=61b6eYkAAAAJ">Prof. Qi Tian</a>, <a href="https://scholar.google.com.hk/citations?user=Ud6aBAcAAAAJ&hl=zh-CN&oi=ao">Dr. Xiaopeng Zhang</a>, <a href="https://scholar.google.com.hk/citations?user=RDwnNsQAAAAJ&hl=zh-CN&oi=ao">Dr. Jianlong Chang</a>, and <a href="https://scholar.google.com.hk/citations?user=hCr8Km8AAAAJ&hl=zh-CN&oi=ao">Dr. Peisen Zhao</a>.

My primary research interests lie in Vision-Language-Audio Multi-modal Learning, Video Understanding/Generation, and Music Composition.

As a young researcher, your interest and kind citation will definitely mean a lot for me and my collaborators.

**Email:** ju_chen[at]sjtu[dot]edu[dot]cn / cju[dot]void[at]gmail[dot]com    
 


# ğŸ”¥ News
<!-- - [*2023*] ![new paper](/images/new.gif) One paper is accepted to DASFAA 2023, about table-and-text question answering. -->
- [*2023*] ![new paper](/images/new.gif) Our new preprints, [Multi-modal Prompt Learning](https://arxiv.org/pdf/2212.09335.pdf) for vision-language foundation models is out!
- [*2023*] ![new paper](/images/new.gif) Our new preprints, [Collaborative Distillation](https://arxiv.org/pdf/2212.09335.pdf) so that multiple pre-trainings complement each other is out!
- [*2022*] One paper is accepted to ECCV 2022, about efficient adaptation of VLP.


# ğŸ“ Publications 
Please feel free to drop me an email for any suggestions or potential collaborations.
### ğŸ“’ Topic: Efficiently Adapt Multi-modal Foundation Models to Unify/Generalize Downstream Tasks
1. [Prompting Visual-Language Models for Efficient Video Understanding](https://arxiv.org/pdf/2112.04478.pdf) \| [[Code & Data](https://github.com/ju-chen/Efficient-Prompt)]  \|  [[Project](https://ju-chen.github.io/efficient-prompt/)]  \| [[Report](https://github.com/ju-chen/Efficient-Prompt/tree/main/datasplits/)]        
**Chen Ju**, Tengda Han, Kunhao Zheng, Ya Zhang and Weidi Xie  
In Proc. of **ECCV 2022**  

1. [Distilling Vision-Language Pre-training to Collaborate with Weakly-Supervised Temporal Action Localization](https://arxiv.org/pdf/2212.09335.pdf)  \|  [[Project](https://ju-chen.github.io/efficient-prompt/)]     
**Chen Ju**, Kunhao Zheng, Jinxiang Liu, Peisen Zhao, Ya Zhang, Jianlong Chang, Yanfeng Wang and Qi Tian 
In Proc. of **arXiv preprint 2023** 

1. [Multi-modal Prompt Learning to Bridge Optical Flow for
Low-Shot Video Understanding](https://arxiv.org/pdf/2212.09335.pdf)   \|  [[Project](https://ju-chen.github.io/efficient-prompt/)]    
**Chen Ju**, Zeqian Li, Peisen Zhao, Ya Zhang, Qi Tian and Weidi Xie
In Proc. of **arXiv preprint 2023** 


### ğŸ“’ Topic: Vision-Language-Audio Pre-trainings with Strong Generalization but Low Costs
1. [Long-Document Cross-Lingual Summarization](https://arxiv.org/abs/2212.00586) \| [[Data](https://github.com/LearnItBoy/Perseus)] \| [[Report](https://mp.weixin.qq.com/s/klalyNLgNRx6-cvxtTJ3OA)]    
Shaohui Zheng, Zhixu Li, **Jiaan Wang**, Jianfeng Qu, An Liu, Lei Zhao and Zhigang Chen  
To appear in **WSDM 2023** (full paper)  
1. A Joint Link-Retrieve Framework for Open Table-and-Text Question Answering  
Jian Zou, **Jiaan Wang**, Ying He, Jianfeng Qu, Zhixu Li, Pengpeng Zhao, An Liu and Lei Zhao  
To appear in **DASFAA 2023** (full paper)


### ğŸ“’ Topic: Freeze Pre-trainings, Downstream Video Understanding with Limited Annotation & Supervision
1. [A Survey on Cross-Lingual Summarization](https://arxiv.org/abs/2203.12515) \| [[Report](https://mp.weixin.qq.com/s/OL6-Yp4NgnTsvXMCHOJMog)]    
**Jiaan Wang**, Fandong Meng<sup>*</sup>, Duo Zheng, Yunlong Liang, Zhixu Li<sup>*</sup>, Jianfeng Qu and Jie Zhou   
In Proc. of **TACL 2022**   
1. [ClidSum: A Benchmark Dataset for Cross-Lingual Dialogue Summarization](https://arxiv.org/abs/2202.05599) \| [[Data&Code](https://github.com/krystalan/ClidSum)] \| [[Models](https://huggingface.co/Krystalan)] \| [[Report](https://mp.weixin.qq.com/s/M8BR3MySZBuu7ixdFi_SRQ)]   
**Jiaan Wang**, Fandong Meng<sup>*</sup>, Ziyao Lu, Duo Zheng, Zhixu Li, Jianfeng Qu and Jie Zhou   
In Proc. of **EMNLP 2022** (full paper) 
1. [Towards Unifying Reference Expression Generation and Comprehension](https://arxiv.org/abs/2210.13076) \| [[Code](https://github.com/zd11024/UniRef)] \| [[Report](https://mp.weixin.qq.com/s/jwiDN9N_sgs32Wjt2-a-kg)]    
Duo Zheng, Tao Kong, Ya Jing, **Jiaan Wang** and Xiaojie Wang<sup>*</sup>    
In Proc. of **EMNLP 2022** (full paper)
1. [Knowledge Enhanced Sports Game Summarization](https://arxiv.org/abs/2111.12535) \| [[Data&Code](https://github.com/krystalan/K-SportsSum)]   
**Jiaan Wang**, Zhixu Li<sup>*</sup>, Tingyi Zhang, Duo Zheng, Jianfeng Qu<sup>*</sup>, An Liu, Lei Zhao and Zhigang Chen  
In Proc. of **WSDM 2022** (full paper)   
1. [Incorporating Commonsense Knowledge into Story Ending Generation via Heterogeneous Graph Networks](https://arxiv.org/abs/2201.12538) \| [[Code](https://github.com/krystalan/AwesomeSEG)]    
**Jiaan Wang**, Beiqi Zou, Zhixu Li, Jianfeng Qu<sup>*</sup>, Pengpeng Zhao, An Liu and Lei Zhao   
In Proc. of **DASFAA 2022** (full paper) 
1. [RT-KGD: Relation Transition Aware Knowledge-Grounded Dialogue Generation](https://arxiv.org/abs/2207.08212) \| [[Code](https://github.com/tigerwww-git/RT-KGD)]   
Kexin Wang, Zhixu Li<sup>*</sup>, **Jiaan Wang**, Jianfeng Qu<sup>*</sup>, Ying He, An Liu and Lei Zhao   
In Proc. of **ISWC 2022** (full paper)
1. [LayerConnect: Hypernetwork-Assisted Inter-Layer Connector to Enhance Parameter Efficiency](https://aclanthology.org/2022.coling-1.276/)   
Haoxiang Shi, Rongsheng Zhang, **Jiaan Wang**, Cen Wang, Guandan Chen, Yinhe Zheng and Tetsuya Sakai    
In Proc. of **COLING 2022** (short paper)
1. [Aligning Internal Regularity and External Influence of Multi-Granularity for Temporal Knowledge Graph Embedding](https://link.springer.com/chapter/10.1007/978-3-031-00129-1_10)   
Tingyi Zhang, Zhixu Li, **Jiaan Wang**, Jianfeng Qu<sup>*</sup>, Lin Yuan, An Liu, Lei Zhao and Zhigang Chen   
In Proc. of **DASFAA 2022** (full paper)


### ğŸ“’ 2021
1. [SportsSum2.0: Generating High-Quality Sports News from Live Text Commentary](http://arxiv.org/abs/2110.05750) \| [[Data](https://github.com/krystalan/SportsSum2.0)]   
**Jiaan Wang**, Zhixu Li<sup>*</sup>, Qiang Yang, Jianfeng Qu, Zhigang Chen, Qingsheng Liu and Guoping Hu   
In Proc. of **CIKM 2021** (short paper)    
1. [Multi-Modal Chorus Recognition for Improving Song Search](https://arxiv.org/abs/2106.16153) \| [[Data&Code](https://github.com/krystalan/MMCR)]    
**Jiaan Wang**, Zhixu Li<sup>*</sup>, Binbin Gu, Tingyi Zhang, Qingsheng Liu and Zhigang Chen  
In Proc. of **ICANN 2021** (full paper)
1. [ã€ŠSGSum: ä¸€ä¸ªé¢å‘ä½“è‚²èµ›äº‹æ‘˜è¦çš„äººå·¥æ ‡æ³¨æ•°æ®é›†ã€‹](https://wangjiaan.cn/files/SGSum.pdf) \| [[Data](https://github.com/krystalan/SGSum)] \| [[Report1](https://mp.weixin.qq.com/s/WENz9fX5HN4agBCMbYFSCQ)] \| [[Report2](https://mp.weixin.qq.com/s/-qgu12c1MpGPH6iT-JW1Cg)]  
**Jiaan Wang**, Tingyi Zhang, Jianfeng Qu, Qingsheng Liu, Zhigang Chen, Zhixu Li<sup>*</sup>   
**CCKS 2021** (resource track paper)  <font color=red> (Best Paper Nominee) </font>  
1. [Enhancing Visual Dialog Questioner with Entity-based Strategy Learning and Augmented Guesser](https://arxiv.org/abs/2109.02297) \| [[Code](https://github.com/zd11024/Entity_Questioner)]    
Duo Zheng, Zipeng Xu, Fandong Meng, Xiaojie Wang<sup>*</sup>, **Jiaan Wang** and Jie Zhou    
In Proc. of **EMNLP 2021 Findings** (full paper)   



# ğŸ—ï¸ Academic Services
- PC Member & Reviewer: ICCV 2023, CVPR 2023, AAAI 2023, ECCV 2022



# ğŸ“„ Patents
- Ya Zhang, **<u>Chen Ju</u>**, Haicheng Wang, Jinxiang Liu, Chaofan Ma, Yanfeng Wang. [ã€Šå±€éƒ¨ç›‘ç£é•¿è§†é¢‘æ—¶åºæ–‡æœ¬æ£€ç´¢æ–¹æ³•åŠç³»ç»Ÿã€‹](https://cprs.patentstar.com.cn/Search/Detail?ANE=9HEE9IFE9GDC9FDB5AEA9IDC9HHF9CIG7FCA9AFF9EDE9IBD) CN202211581256.7
- Ya Zhang, **<u>Chen Ju</u>**, Kunhao Zheng, Jinxiang Liu, Weidi Xie, Yanfeng Wang. [ã€Šå¼±ç›‘ç£è§†é¢‘æ—¶åºåŠ¨ä½œæ£€æµ‹ä¸åˆ†ç±»æ–¹æ³•åŠç³»ç»Ÿã€‹](https://cprs.patentstar.com.cn/Search/Detail?ANE=9HEE9IFE9GDC9FDB5AEA9IDC9HHF9CIG7FCA9AFF9EDE9IBD) CN202211056034.3
- Ya Zhang, **<u>Chen Ju</u>**, Peisen Zhao, Siheng Chen, Xiaoyun Zhang, Yanfeng Wang. [ã€Šä¸€ç§å•å¸§ç›‘ç£è§†é¢‘æ—¶åºåŠ¨ä½œæ£€æµ‹ä¸åˆ†ç±»çš„æ–¹æ³•åŠç³»ç»Ÿã€‹](https://cprs.patentstar.com.cn/Search/Detail?ANE=9HEE9IFE9GDC9FDB5AEA9IDC9HHF9CIG7FCA9AFF9EDE9IBD) CN202111190861.7
- Ya Zhang, **<u>Chen Ju</u>**, Yanfeng Wang. [ã€Šä¸€ç§å¼±ç›‘ç£è§†é¢‘æ—¶åºåŠ¨ä½œæ£€æµ‹ä¸ç±»åˆ«é¢„æµ‹çš„æ–¹æ³•åŠç³»ç»Ÿã€‹](https://cprs.patentstar.com.cn/Search/Detail?ANE=9HEE9IFE9GDC9FDB5AEA9IDC9HHF9CIG7FCA9AFF9EDE9IBD) CN202010403823.4



# ğŸ“– Educations
- *2018.09 - Now *, PhD, Shanghai Jiao Tong University, Shanghai, China
- *2014.09 - 2018.06*, Undergraduate, University Of Electronic Science And Technology Of China, Chengdu, China



# ğŸ– Honors and Awards
- [*2020 & 2022*] CMIC Outstanding Scholarship at SJTU
- [*2018*] Outstanding Graduates of Sichuan Province
- [*2016 & 2017*] Undergraduate National Scholarship at UESTC



