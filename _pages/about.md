---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

<span class='anchor' id='about-me'></span>

Hello! Iâ€™m Chen Ju (é é™ˆ).

I'm a final-year PhD candidate at <a href="https://mediabrain.sjtu.edu.cn/">MediaBrain Group</a>, Shanghai Jiao Tong University, advised by <a href="https://scholar.google.com.hk/citations?user=x_sgJskAAAAJ&hl=zh-CN">Prof. Yanfeng Wang</a> (ä¸Šæµ·AI LABä¸»ä»»åŠ©ç†) and <a href="https://mediabrain.sjtu.edu.cn/yazhang/">Prof. Ya Zhang</a> (å›½å®¶ä¸‡äºº), also collaborating with <a href="https://weidixie.github.io/">Prof. Weidi Xie</a>, <a href="https://siheng-chen.github.io/">Prof. Siheng Chen</a>, <a href="https://mediabrain.sjtu.edu.cn/yuwang/">Prof. Yu Wang</a> and <a href="https://sunarker.github.io/index.html">Prof. Jiangchao Yao</a>. 
Before that, I obtained a Bachelor's degree in Engineering from University of Electronic Science and Technology of China, where I studied under <a href="https://scholar.google.com.hk/citations?user=r0aZUfoAAAAJ&hl=zh-CN&oi=ao">Prof. Yong Liu</a> (å›½å®¶æ°é’ & é•¿æ±Ÿå­¦è€…), awarded with the honor of Outstanding Graduate.


Currently, I collaborate closely with some outstanding researchers from TAO Technology (æ‹ç«‹æ·˜), Alibaba: <a href="https://scholar.google.com.hk/citations?user=78vU1IUAAAAJ&hl=zh-CN&oi=ao">Dr. Weilin Huang</a>, <a href="https://scholar.google.com.hk/citations?user=qBTDCawAAAAJ&hl=zh-CN&oi=ao">Dr. Shuai Xiao</a>, <a href="https://scholar.google.com.hk/citations?user=6Qa2JCwAAAAJ&hl=zh-CN">Dr. Xu Chen</a>, and <a href="https://scholar.google.com.hk/citations?user=o4SDCAYAAAAJ&hl=zh-CN&oi=ao">Dr. Zhonghua Zhai</a>. 
The vision is to develop large-scale visual searching system for various e-commerce applications, such as superlarge-scale multi-modal learning (10-billion image-text product data), AIGC (GPT & Diffusion).


Before, I study with some outstanding researchers from WeChat Technology (å¾®ä¿¡æŠ€æœ¯æ¶æ„), Tencent: <a href="https://scholar.google.com.hk/citations?user=38dACd4AAAAJ&hl=zh-CN&oi=ao">Dr. Fengyun Rao</a>, <a href="https://scholar.google.com.hk/citations?user=dHBNmSkAAAAJ&hl=zh-CN&oi=ao">Dr. Yizhou Zhou</a>, <a href="https://scholar.google.com.hk/citations?user=cKY8e8sAAAAJ&hl=zh-CN&oi=ao">Dr. Guangting Wang</a> and <a href="https://scholar.google.com.hk/citations?user=O00rbxoAAAAJ&hl=zh-CN&oi=sra">Dr. Yukun Su</a>, working to develop chinese pre-trainings of image-text-video-music, namely WeMM, WeCLIP, WeMU.


Earlier, I cooperate with some outstanding researchers from PanGu Large Model (ç›˜å¤å¤§æ¨¡å‹), Huawei: <a href="https://scholar.google.com/citations?user=61b6eYkAAAAJ">Prof. Qi Tian</a>, <a href="https://scholar.google.com/citations?user=EEMm7hwAAAAJ&hl=zh-CN&oi=ao">Dr. Lingxi Xie</a>, <a href="https://scholar.google.com.hk/citations?user=Ud6aBAcAAAAJ&hl=zh-CN&oi=ao">Dr. Xiaopeng Zhang</a>, <a href="https://scholar.google.com.hk/citations?user=RDwnNsQAAAAJ&hl=zh-CN&oi=ao">Dr. Jianlong Chang</a>, <a href="https://scholar.google.com.hk/citations?user=-JcFoOoAAAAJ&hl=zh-CN&oi=ao">Dr. Jiemin Fang</a>, and <a href="https://scholar.google.com.hk/citations?user=hCr8Km8AAAAJ&hl=zh-CN&oi=ao">Dr. Peisen Zhao</a>, to explore MLLM for B-side industrial scenarios.


Iâ€™m now leading one small group that mainly works on <b> Efficient Data Governance (Cleaner; Organizer; Compressor; Distiller; Synthesizer; Evolver)</b>. Actively recruiting research interns and engineering interns, please feel free to contact me!


<p> <b>Email:</b> ju_chen[at]sjtu[dot]edu[dot]cn / cju[dot]void[at]gmail[dot]com         &emsp; &emsp; &emsp; &emsp; &emsp;             
<b> Google Scholar:</b> Citations 630+, H-index 10, I10-index 9 </p>  



# ğŸ”¥ News
- [*2024.04*] ![new paper](/images/new.gif) Our new work, [rethinking the robustness for open-vocabulary visual understanding](https://arxiv.org/pdf/2404.14890.pdf) is out!
- [*2024.04*] ![new paper](/images/new.gif) Our new work, [wear-any-way: manipulable virtual try-on via sparse correspondence alignment](https://arxiv.org/pdf/2403.12965.pdf) is out!
- [*2024.03*] ![new paper](/images/new.gif) Our new work, [vision-audio-text alignment, from large-scale self-supervised video streaming](https://arxiv.org/pdf/2403.11074.pdf) is out!
- [*2024.01*] Remarking for 500 Citations in Google Scholar.
- [*2023.12*] Our work, [universal VLMs acceleration architecture, from one novel perspective of data de-redundancy](https://arxiv.org/pdf/2312.07408.pdf) is out!
- [*2023.09*] Our work, [diversifying semantics' attributes via LLMs for open-set visual system](https://arxiv.org/pdf/2309.00096.pdf) is out!
- [*2023.07*] Our work, [aligning LLMs' remarkable semantics for multi-modal understanding system](https://arxiv.org/pdf/2307.02003.pdf) is out!
- [*2023.05*] Our work, [distilling fine-grained priors from stable diffusion for unsupervised object discovery](https://arxiv.org/pdf/2303.09813.pdf) is out!
- [*2023.04*] Our work, [multi-modal GPT prompting for vision-language foundation models](https://arxiv.org/pdf/2303.11732.pdf) is out!
- [*2023.03*] Our work, [collaborative distillation so that multiple foundation pre-trainings complement each other](https://arxiv.org/pdf/2212.09335.pdf) is out!
- [*2023.02*] Our work, [partial supervision with quadruple contrasts for cost-effective vision-language pre-training](https://arxiv.org/pdf/2302.09850.pdf) is out!
- One paper is accepted to CVPR 2024, about audio-visual segmentation via unlabeled frame exploitation.
- One paper is accepted to WWW 2024, about cross-domain CTR prediction via explicit feature augmentation.
- One paper is accepted to NIPS 2023, about general semantic understanding for multi-modal large models.
- One paper is accepted to ICCV 2023, about finer visual understanding from multiple diffusion models.
- One paper is accepted to CVPR 2023, about effective collaboration of multiple foundation models.
- One paper is accepted to ECCV 2022, about efficient adaptation for vision-language foundation models.  
- One paper is accepted to ACM Multimedia 2022, about cost-effective pre-training for video-audio foundation models.     


# ğŸ’» Researches
My primary research interests lie in 

- **Vision-Language-Music Learning**: Multi-Modal Pre-training, Efficient Adaptation, Accelerate Deployment for Downstream Tasks.

- **Data Governance & Mining**: Clean & Compress & Synthesize Data, Cross-Modal Retrieval & Recommendation & Advert. 

- **AIGC**ï¼šGeneration or Editing for Image & Video & Music, Conversation-Driven Multi-Modal Understanding and Composition.

- **Video Understanding**: Retrieval & Caption & Summary for Video Clips, Detection & Classification for Untrimmed Long Videos.

As a young researcher, your interest and kind citation will definitely mean a lot for me and my collaborators. Also feel free to drop me an email for any suggestions or potential collaborations.



# ğŸ“ Publications 
### ğŸ“’ Topic: Efficiently Adapt Multi-modal Foundation Models to Unify/Generalize Downstream Tasks
1. [Prompting Visual-Language Models for Efficient Video Understanding](https://arxiv.org/pdf/2112.04478.pdf)  \|  [[Project](https://ju-chen.github.io/efficient-prompt/)]  \| [[Code & Data](https://github.com/ju-chen/Efficient-Prompt)]  \| [[Report](https://mp.weixin.qq.com/s/F8RGa0IQyljfue3fAxvATw)]  \| [[Bibtex](./CITE/cite_prompt.txt)]                  
**Chen Ju**, Tengda Han, Kunhao Zheng, Ya Zhang and Weidi Xie  
**ECCV 2022**  

1. [Collaborating Vision-Language Pre-training with Weakly-Supervised Video Understanding](https://arxiv.org/pdf/2212.09335.pdf)  \|  [[Project & Code](https://voide1220.github.io/distillation_collaboration/)]  \| [[Bibtex](./CITE/cite_distilling.txt)]                                 
**Chen Ju**, Kunhao Zheng, Jinxiang Liu, Peisen Zhao, Ya Zhang, Jianlong Chang, Qi Tian and Yanfeng Wang      
**CVPR 2023** 

1. [Turbo: Informativity-Driven Acceleration Plugin for Vision-Language Foundation Models](https://arxiv.org/pdf/2312.07408.pdf)   \| [[Bibtex](./CITE/cite_turbo.txt)]                       
**Chen Ju**, Haicheng Wang, Zeqian Li, Xu Chen, Zhonghua Zhai, Weilin Huang and Shuai Xiao             
arXiv preprint 2023     



### ğŸ“’ Topic: Vision-Language-Audio Pre-trainings & Inference with Strong Generalization but Low Costs
1. [Transformation Invariance and Equivariance for Self-supervised Sound Localization](https://arxiv.org/pdf/2206.12772.pdf)  \| [[Project & Demo](https://jinxiang-liu.github.io/SSL-TIE/)] \| [[Code](https://github.com/jinxiang-liu/SSL-TIE)] \| [[Bibtex](./CITE/cite_audio.txt)]                              
Jinxiang Liu, **Chen Ju**, Weidi Xie and Ya Zhang         
**ACM Multimedia 2022**

1. [Audio-Visual Segmentation via Unlabeled Frames Exploitation](https://arxiv.org/pdf/2403.11074.pdf)   \|     [[Bibtex](./CITE/cite_unlabel.txt)]                           
Jinxiang Liu, Yikun Liu, Fei Zhang, **Chen Ju**, Yanfeng Wang and Ya Zhang            
**CVPR 2024** 

1. [Contrast and Unity for Partially-Supervised Temporal Sentence Grounding](https://arxiv.org/pdf/2302.09850.pdf)  \|   [[Project & Code](https://voide1220.github.io/constraint_union/)]  \|    [[Bibtex](./CITE/cite_partial.txt)]            
**Chen Ju**, Haicheng Wang, Jinxiang Liu, Chaofan Ma, Ya Zhang         
arXiv preprint 2023

1. [SAM Guided Annotation-free Audio-Visual Cross-modal Segmentation](https://arxiv.org/pdf/2305.11019.pdf)  \|    [[Project & Code](https://jinxiang-liu.github.io/anno-free-AVS/)] \| [[Bibtex](./CITE/cite_aavs.txt)]           
Jinxiang Liu, Yu Wang, **Chen Ju**, Chaofan Ma, Ya Zhang, Weidi Xie       
**WACV 2024** 



### ğŸ“’ Topic: Understand World through Open-Vocabulary Learning, and also Rethinking Limitations
1. [Multi-modal GPT Prompts for Open-Vocabulary Video Understanding](https://arxiv.org/pdf/2303.11732.pdf) \|  [[Project & Code](https://voide1220.github.io/GPT_prompt/)]  \| [[Bibtex](./CITE/cite_map.txt)]                                    
**Chen Ju**, Zeqian Li, Peisen Zhao, Ya Zhang, Xiaopeng Zhang, Qi Tian, Yanfeng Wang and Weidi Xie     
**Springer IJCV**

1. [Open-Vocabulary Semantic Segmentation via Attribute Decomposition-Aggregation](https://arxiv.org/pdf/2307.02003.pdf) \| [[Bibtex](./CITE/cite_attribute.txt)]                  
Chaofan Ma, Yuhuan Yang, **Chen Ju**, Fei Zhang, Ya Zhang and Yanfeng Wang             
**NIPS 2023**

1. [Multi-Modal Prototypes for Open-Set Semantic Segmentation](https://arxiv.org/pdf/2307.02003.pdf) \| [[Bibtex](./CITE/cite_prototype.txt)]                    
Yuhuan Yang, Chaofan Ma, **Chen Ju**, Ya Zhang and Yanfeng Wang             
arXiv preprint 2023

1. [DENOISER: Rethinking the Robustness for Open-Vocabulary Action Recognition](https://arxiv.org/pdf/2404.14890.pdf) \| [[Bibtex](./CITE/cite_robust.txt)]                                   
Haozhe Cheng, **Chen Ju**, Haicheng Wang, Jinxiang Liu, Mengting Chen, Qiang Hu, Xiaoyun Zhang and Yanfeng Wang            
arXiv preprint 2024



### ğŸ“’ Topic: Innovative AIGC Creativity, Free Vision-Text-Audio Editing and Composition
1. [DiffusionSeg: Adapting Diffusion Towards Unsupervised Object Discovery](https://arxiv.org/pdf/2303.09813.pdf) \| [[Bibtex](./CITE/cite_diffusion.txt)]               
Chaofan Ma, Yuhuan Yang, **Chen Ju**, Fei Zhang, Jinxiang Liu, Yu Wang, Ya Zhang and Yanfeng Wang                 
**ICCV 2023**

1. [Wear-Any-Way: Manipulable Virtual Try-on via Sparse Correspondence Alignment](https://arxiv.org/pdf/2403.12965.pdf) \| [[Project](https://mengtingchen.github.io/wear-any-way-page/)]  \| [[Bibtex](./CITE/cite_wear.txt)]               
Mengting Chen, Xi Chen, Zhonghua Zhai, **Chen Ju**, Xuewen Hong, Jinsong Lan and Shuai Xiao                
arXiv preprint 2024



### ğŸ“’ Topic: Freeze Pre-trainings, Downstream Video Understanding with Limited Annotation & Supervision
1. [Divide and Conquer for Single-frame Temporal Action Localization](https://openaccess.thecvf.com/content/ICCV2021/papers/Ju_Divide_and_Conquer_for_Single-Frame_Temporal_Action_Localization_ICCV_2021_paper.pdf)  \|  [[Project & Demo](https://voide1220.github.io/divide_conquer/)]   \|  [[Bibtex](./CITE/cite_divide.txt)]             
**Chen Ju**, Peisen Zhao, Siheng Chen, Ya Zhang, Yanfeng Wang and Qi Tian                
**ICCV 2021** 

1. [Bottom-Up Temporal Action Localization with
Mutual Regularization](https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123530528.pdf) \| [[Demo](https://voide1220.github.io/adapter/)]  \|   [[Code](https://github.com/PeisenZhao/Bottom-Up-TAL-with-MR)]   \|  [[Bibtex](./CITE/cite_bottom.txt)]               
Peisen Zhao, Lingxi Xie, **Chen Ju**, Ya Zhang, Yanfeng Wang and Qi Tian               
**ECCV 2020**

1. [Adaptive Mutual Supervision for
Weakly-Supervised Temporal Action Localization](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9920676)  \|  [[Project & Demo](https://voide1220.github.io/adapter/)]  \|   [[Bibtex](./CITE/cite_adaptive.txt)]            
**Chen Ju**, Peisen Zhao, Siheng Chen, Ya Zhang, Xiaoyun Zhang and Qi Tian               
**IEEE Transactions on Multimedia** 

1. [Audio-Aware Query-Enhanced Transformer for Audio-Visual Segmentation](https://arxiv.org/abs/2307.13236.pdf)   \|    [[Project & Code](https://jinxiang-liu.github.io/anno-free-AVS/)]  \|  [[Bibtex](./CITE/cite_aqt.txt)]              
Jinxiang Liu, **Chen Ju**, Chaofan Ma, Yanfeng Wang, Yu Wang, Ya Zhang    
arXiv preprint 2023



### ğŸ“’ Topic: MLLMs Guided Multi-Modal Information Retrieval & Sorting & Recall & Representation
1. [Enhancing Cross-domain Click-Through Rate Prediction via Explicit Feature Augmentation](https://arxiv.org/pdf/2312.00078.pdf)          \|  [[Bibtex](./CITE/cite_cctr.txt)]                            
Xu Chen, Zida Cheng, Jiangchao Yao, **Chen Ju**, Weilin Huang, Xiaoyi Zeng and Shuai Xiao             
**WWW 2024**

1. [Image to Multi-Modal Retrieval Learning for Industrial Scenarios](https://arxiv.org/pdf/2305.03972.pdf)      \|  [[Bibtex](./CITE/cite_immr.txt)]               
Zida Cheng, **Chen Ju**, Xu Chen, Zhonghua Zhai, Shuai Xiao and Junchi Yan           
arXiv preprint 2023   

1. Cell Variational Information Bottleneck Network           
Zhonghua Zhai, **Chen Ju**, Shuai Xiao, Jinsong Lan and Xiaoyi Zeng        
arXiv preprint 2023 



# ğŸ—ï¸ Academics and Communications
- PC Member & Conference Reviewer: ECCV 2024/2022, CVPR 2024/2023, AAAI 2024/2023, ICCV 2023, ACM MM 2024/2023, WACV 2024
- Journal Reviewers: IEEE T-PAMI, Springer IJCV, IEEE T-MM, IEEE TCSVT, NPL

- I am fortunate to meet many interesting people & Team:

1. University System. &emsp;
UESTC: <a href="https://scholar.google.com.hk/citations?user=r0aZUfoAAAAJ&hl=zh-CN&oi=ao">Yong Liu</a>, <a href="https://scholar.google.com.hk/citations?user=r0aZUfoAAAAJ&hl=zh-CN&oi=ao">Yadong Jiang</a>.
&emsp; 
PKU:
<a href="https://scholar.google.com.hk/citations?user=4CQKG8oAAAAJ&hl=zh-CN&oi=ao">Hong Liu</a>, <a href="https://scholar.google.com.hk/citations?user=pXAf0agAAAAJ&hl=zh-CN&oi=ao">Jin Luo</a>, <a href="https://scholar.google.com.hk/citations?user=pXAf0agAAAAJ&hl=zh-CN&oi=ao">Donglin Liu</a>, Yong Peng.
&emsp;
THU: <a href="https://scholar.google.com.hk/citations?user=pXAf0agAAAAJ&hl=zh-CN&oi=ao">Shousheng Han</a>, <a href="https://scholar.google.com.hk/citations?user=pXAf0agAAAAJ&hl=zh-CN&oi=ao">Zhengsong Wang</a>, Zongren Dai. 
&emsp; 
SJTU: <a href="https://scholar.google.com.hk/citations?user=x0Uk7S8AAAAJ&hl=zh-CN&oi=ao">Haicheng Wang</a>, <a href="https://scholar.google.com.hk/citations?user=wSRKaWIAAAAJ&hl=zh-CN&oi=ao">Jinxiang Liu</a>, <a href="https://scholar.google.com.hk/citations?user=XBbwb78AAAAJ&hl=zh-CN&oi=ao">Yue Hu</a>, <a href="https://scholar.google.com.hk/citations?user=UgmMqV0AAAAJ&hl=zh-CN&oi=ao">Chenxin Xu</a>, <a href="https://scholar.google.com.hk/citations?user=BAZSE7wAAAAJ&hl=zh-CN&oi=ao">Chaoqin Huang</a>, <a href="https://scholar.google.com.hk/citations?user=Zno4WggAAAAJ&hl=zh-CN&oi=ao">Xiaoman Zhang</a>, <a href="https://scholar.google.com.hk/citations?user=JuRztWYAAAAJ&hl=zh-CN&oi=ao">Xuehui Wang</a>, Jiazhong Ceng, Chen Yang.
&emsp;
USTC: Jiaqing Gao, Yumin Xia, Qi Meng.
&emsp;
Oxford: <a href="https://tengdahan.github.io/">Tengda Han</a>, <a href="https://charigyang.github.io/">Charig Yang</a>.
&emsp;
KU Leuven: Haien Tang, Chunzhuo Wang, Liting Yang.
&emsp;
NUS: <a href="https://scholar.google.com.hk/citations?user=sj4FqEgAAAAJ&hl=zh-CN&oi=ao">Jialin Gao</a>
&emsp; 
OpenGVLab: <a href="https://yangxue0827.github.io/">Xue Yang</a>.
&emsp; 
Ruijin: <a href="https://scholar.google.com.hk/citations?user=hQJ5hwMAAAAJ&hl=zh-CN&oi=ao">Qinwei Xu</a>


3. Alibaba. &emsp;
TAO Technology: Zida Cheng, Mengting Chen, Xuewen Hong, Yixuan Huang, Lianyu Du.
&emsp;
DAMO Academy:
<a href="https://scholar.google.com.hk/citations?user=QeSoG3sAAAAJ&hl=zh-CN&oi=sra">Chang Zhou</a>, <a href="https://xavierchen34.github.io/">Xi Chen</a>, <a href="https://scholar.google.com.hk/citations?user=6bTGGDAAAAAJ&hl=zh-CN&oi=ao">Mosha Chen</a>.
&emsp;
Alimama: Jiajie Wang, Hao Wu, Yuanzhe Gu.
&emsp;
T-head: Yu Fu, He Guo.
&emsp;
AntGroup: <a href="https://scholar.google.com.hk/citations?user=6FsgWBMAAAAJ&hl=zh-CN&oi=ao">Tong Zhan</a>, <a href="https://scholar.google.com.hk/citations?user=11HDEbkAAAAJ&hl=zh-CN&oi=ao">Qingpei Guo</a>, Yifei Hu,
<a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=uBHJx08AAAAJ&view_op=list_works&sortby=pubdate">Ming Yang</a>, <a href="https://scholar.google.com.hk/citations?user=8SCEv-YAAAAJ&hl=zh-CN&oi=ao">Jingdong Chen</a>. 

5. Huawei. &emsp; 
Cloud BU: <a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=GRcH3nAAAAAJ">Yucheng Liu</a>,
<a href="https://scholar.google.com.hk/citations?user=JmEBaccAAAAJ&hl=zh-CN&oi=ao">Yaoming Wang</a>,
<a href="https://scholar.google.com.hk/citations?user=jmFZZYMAAAAJ&hl=zh-CN&oi=ao">Shuangrui Ding</a>,
<a href="https://scholar.google.com.hk/citations?user=9nqZkmUAAAAJ&hl=zh-CN&oi=ao">Haohang Xu</a>. 
&emsp;
Car BU: <a href="https://scholar.google.com.hk/citations?user=Qkx2FKoAAAAJ&hl=zh-CN&oi=ao">Maosen Li</a>. 
&emsp;
Consumer BG: Yongli Jia, <a href="https://phellonchen.github.io/">Feilong Chen</a>, Chenliang Hu. 
&emsp; 
ICT: Liang Zhao, Tongda Li.
&emsp;
2012: Yu Zhou, Guohao Gong.

6. Baidu. &emsp; 
Big Search: <a href="https://scholar.google.com.hk/citations?user=UFhcrs4AAAAJ&hl=zh-CN&oi=ao">Zhengyang Li</a>, Suqi Chen. 
&emsp; 
Ernie Bot: Tian Wu, <a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=9ClaqP0AAAAJ&view_op=list_works&sortby=pubdate">Jiachen Liu</a>.
&emsp;
Phoenix Nest: Chenyang Li.

7. Tencent. &emsp; 
WXG: Xiaoyi Jia, Honghui Lin, Yongsheng Luo, Tianyi Wang, Zhenghua Liu, <a href="https://hellwayxue.github.io/">Dr. Hongwei Xue</a>, <a href="https://scholar.google.com.hk/citations?user=bvSAvkcAAAAJ&hl=zh-CN&oi=ao">Dr. Dacheng Yin</a>.
&emsp; 
CDG: Tianyue Cao.
&emsp;
TEG: <a href="https://scholar.google.com.hk/citations?user=q9Fn50QAAAAJ&hl=zh-CN&oi=ao">Hongfa Wang</a>, <a href="https://scholar.google.com.hk/citations?user=AjxoEpIAAAAJ&hl=zh-CN&oi=ao">Wei Liu</a>.


8. Software Company. &emsp; 
Meta: <a href="https://dyekuu.github.io/">Kunhao Zheng</a>.
&emsp;
DiDi: <a href="https://scholar.google.com.hk/citations?user=CJgDlnoAAAAJ&hl=zh-CN&oi=ao">Zhe Xu</a>.
&emsp;
ByteDance: Yichao Xiong, Zhikang Li, Kunyuan Du, Xuan Liao, Yuxuan Jiang, Shiqi Peng, Hangtian Zhao, Jian Li.
&emsp;
Bilibili: Luochen Lv.
&emsp;
ZTE: Xiao Hu. 
&emsp;
KuaiShou: Liwei Chen, Kun Xu.
&emsp;
MeiTuan: <a href="https://scholar.google.com.hk/citations?user=9rY7WqAAAAAJ&hl=zh-CN&oi=ao">Yujie Zhong</a>, Yexun Zhang.  

  
9. Hardware Company. &emsp;
INVIDIA: <a href="https://scholar.google.com.hk/citations?user=Etmn37YAAAAJ&hl=zh-CN&oi=ao">Jie Chang</a>, <a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=Xv9DcdEAAAAJ">Yangheng Zhao</a>.
&emsp;
Intel: <a href="https://scholar.google.com.hk/citations?user=d7r3NGQAAAAJ&hl=zh-CN&oi=ao">Yujie Pan</a>, Yingying Xue.
&emsp;
Hikvision: Tengfei Hou, Wanshun Gao. 
&emsp;
OPPO: Bo Wang, Chen Chen, Haonan Lu.
&emsp;
Honor: Yuanchao Du.


# ğŸ“„ Patents
- CN202010403823.4 [ã€Šä¸€ç§åŸºäºè‡ªé€‚åº”é‡‡æ ·çš„å¼±ç›‘ç£æ—¶åºåŠ¨ä½œæ£€æµ‹æ–¹æ³•åŠç³»ç»Ÿã€‹](https://cprs.patentstar.com.cn/Search/Detail?ANE=9AIB2ACA7BEA6GAA8HAA8EEA9BFB9HGE9FCB9AED9BGA9AGA)         
Ya Zhang, **<u>Chen Ju</u>**, Yanfeng Wang. 
- CN202111190861.7 [ã€Šä¸€ç§å•å¸§ç›‘ç£è§†é¢‘æ—¶åºåŠ¨ä½œæ£€æµ‹ä¸åˆ†ç±»æ–¹æ³•åŠç³»ç»Ÿã€‹](https://cprs.patentstar.com.cn/Search/Detail?ANE=9IBC9FIE9HFF6BEA6DCA4BCA3ACAACGA9GFCDHFA9DFF6CDA)         
Ya Zhang, **<u>Chen Ju</u>**, Peisen Zhao, Siheng Chen, Xiaoyun Zhang, Yanfeng Wang. 
- CN202211056034.3 [ã€Šå¼±ç›‘ç£è§†é¢‘æ—¶åºåŠ¨ä½œæ£€æµ‹ä¸åˆ†ç±»æ–¹æ³•åŠç³»ç»Ÿã€‹](https://cprs.patentstar.com.cn/Search/Detail?ANE=5CBA3BCAAHIA9GIH9FDA8CFA7FCA9HBEAGFA9CIB9EHFAHGA)       
Ya Zhang, **<u>Chen Ju</u>**, Kunhao Zheng, Jinxiang Liu, Weidi Xie, Yanfeng Wang.  
- CN202211581256.7 [ã€Šå±€éƒ¨ç›‘ç£é•¿è§†é¢‘æ—¶åºæ–‡æœ¬æ£€ç´¢æ–¹æ³•åŠç³»ç»Ÿã€‹](https://cprs.patentstar.com.cn/Search/Detail?ANE=9GHG6CEA9FCA9DEA9IEE9GGE9BGDAIBA9IBB9IBC9EAA9ICH)       
Ya Zhang, **<u>Chen Ju</u>**, Haicheng Wang, Jinxiang Liu, Chaofan Ma, Yanfeng Wang.  
- CN202310913202.4 [ã€ŠåŸºäºå±æ€§åˆ†è§£-èšåˆçš„å¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²æ–¹æ³•åŠç³»ç»Ÿã€‹](https://cprs.patentstar.com.cn/Search/Detail?ANE=9HBB8IAA5CBA9CFB6CDA8EEAFFIA8GBA8FAA9BHB7FCA9EHE)       
Yanfeng Wang, Chaofan Ma, Yuhuan Yang, **<u>Chen Ju</u>**, Fei Zhang, Ya Zhang.  
- CN202410913202.4 [ã€Šä¸€ç§åŸºäºç¨€ç–å…³ç³»å¯¹é½çš„å¯è‡ªç”±æ§åˆ¶çš„è¯•è¡£æ–¹æ³•ã€‹](https://cprs.patentstar.com.cn/Search/Detail?ANE=9HBB8IAA5CBA9CFB6CDA8EEAFFIA8GBA8FAA9BHB7FCA9EHE)       
Mengting Chen, Xi Chen, Zhonghua Zhai, **<u>Chen Ju</u>**, Xuewen Hong, Jinsong Lan, Shuai Xiao. 
- CN202410913202.4 [ã€Šä¸€ç§å¸¦å™ªå¤šæ¨¡æ€å¼€æ”¾è¯æ±‡è§†è§‰æ ·æœ¬åˆ†ç±»æ–¹æ³•åŠç³»ç»Ÿã€‹](https://cprs.patentstar.com.cn/Search/Detail?ANE=9HBB8IAA5CBA9CFB6CDA8EEAFFIA8GBA8FAA9BHB7FCA9EHE)       
Xiaoyun Zhang, Haozhe Cheng, **<u>Chen Ju</u>**, Qiang Hu, Yanfeng Wang. 


# ğŸ“– Educations
- *2018 - 2024*, PhD candidate, Shanghai Jiao Tong University, Shanghai, China
- *2018*, Exchange Student, University of Amsterdam, Netherlands
- *2018*, Exchange Student, KU Leuven, Belgium
- *2014 - 2018*, Undergraduate, University of Electronic Science and Technology of China, Chengdu, China



# ğŸ– Honors and Awards
- [*2024*] Top Talent Program by Technology Companies
- [*2023*] First Prize of Shanghai Technology Invention Award
- [*2022*] CMIC Outstanding Scholarship at SJTU (Top 1%)
- [*2021*] CMIC Outstanding Scholarship at SJTU (Top 1%)
- [*2020*] CMIC Outstanding Scholarship at SJTU (Top 1%)
- [*2018*] Outstanding Graduates of Sichuan Province (Top 1%)
- [*2018*] Outstanding Graduates of UESTC (Top 1%)
- [*2017*] First Prize in National Undergraduate Mathematical Modeling
- [*2017*] Undergraduate National Scholarship at UESTC (Top 1%)
- [*2016*] Undergraduate National Scholarship at UESTC (Top 1%)
- [*2015*] Undergraduate National Scholarship at UESTC (Top 1%)
